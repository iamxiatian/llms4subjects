{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jina ColBert v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_ENDPOINT=https://hf-mirror.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "model_path = \"jinaai/jina-colbert-v2\"\n",
    "model_path = \"/root/xiatian/models/jina-colbert-v2\"\n",
    "RAG = RAGPretrainedModel.from_pretrained(model_path)\n",
    "docs = [\n",
    "    \"ColBERT is a novel ranking model that adapts deep LMs for efficient retrieval.\",\n",
    "    \"Jina-ColBERT is a ColBERT-style model but based on JinaBERT so it can support both 8k context length, fast and accurate retrieval.\",\n",
    "]\n",
    "RAG.index(docs, index_name=\"demo\")\n",
    "query = \"What does ColBERT do?\"\n",
    "results = RAG.search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers运行训练完成的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/llms4subjects-s_Ootod2-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 9/9 [00:08<00:00,  1.10it/s]\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"/root/xiatian/LLaMA-Factory/models/llama3_tibkat_lora_sft\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "dtype = torch.float16\n",
    "device = \"auto\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=dtype, device_map=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128001\n",
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19949/19949 [00:01<00:00, 13565.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from llms4subjects.instance import load_jsonline_file\n",
    "from llms4subjects.prompt import make_input_text\n",
    "\n",
    "eval_ds = load_jsonline_file(\"./db/instance/merged/dev.jsonline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/llms4subjects-s_Ootod2-py3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/.cache/pypoetry/virtualenvs/llms4subjects-s_Ootod2-py3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a librarian responsible for assigning a set of subject tags to a technical document based on its title and abstract. Based on the information provided below, please output the related subjects, with one subject per line.\n",
      "- Title: Sicherung des Familieneinflusses in Familienunternehmen : Symposium der Forschungsstelle für Familienunternehmen der Universität Bayreuth am 6./7. Oktober 2016\n",
      "- Abstract: Der Tagungsband dokumentiert den Stand der wissenschaftlichen Forschung zum Einflusses der Familie in Familienunternehmen und dient zugleich dem Wissenstransfer zwischen Wissenschaft und Praxis. Dazu werden verschiedene Instrumente der Sicherung des Einflusses der Familie in Familienunternehmen von namhaften Experten unter verschiedenen Blickwinkeln analysiert und bewertet. Dies gilt etwa für die Sicherung des Familieneinflusses durch Beiräte, Stiftungen, Interim-Management, notarielle Verträge und gemeinsame Werte. Neben dieser nationalen Betrachtung enthält der Band auch eine vergleichende Darstellung dieses Themas mit Blick auf österreichische, schweizerische und chinesische Familienunternehmen. Sie macht deutlich, dass die Sicherung des Einflusses der Familie in Familienunternehmen eine strukturelle, übergreifende Herausforderung ist – eine Herausforderung, der Praxis und Wissenschaft gemeinsam begegnen müssen.Mit Beiträgen vonProf. Dr. Dr. Alexander Brink, Prof. Dr. Jens Prütting, Prof. Dr. Katharina Uffmann, Prof. Dr. Hermut Kormann, Prof. Dr. Knut Werner Lange, Prof. Dr. Susanne Kalss, Dr. Miriam Bird, Prof. Dr. Reinhard Meckl\n",
      "\n",
      "==========\n",
      "Familie (Unternehmensleitung)\n",
      "Einflußnahme\n"
     ]
    }
   ],
   "source": [
    "doc = eval_ds[3]\n",
    "input_text = make_input_text(doc[\"title\"], doc[\"abstract\"])\n",
    "\n",
    "device = model.device\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "max_length = len(input_ids[0])+1000\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            max_length=max_length,\n",
    "                            repetition_penalty=1.2,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            num_return_sequences=1,\n",
    "                            do_sample=False)\n",
    "out = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(input_text)\n",
    "print(\"==========\")\n",
    "out_text = out[len(input_text):].strip()\n",
    "print(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 观察验证机和训练集文件是否有交集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41902\n",
      "41902\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "train_path = Path(\"./data/shared-task-datasets/TIBKAT/tib-core-subjects/data/train/\")\n",
    "dev_path = Path(\"./data/shared-task-datasets/TIBKAT/tib-core-subjects/data/dev/\")\n",
    "core_files = list(train_path.glob(\"**/*.jsonld\"))\n",
    "core_file_names = [f.name for f in core_files]\n",
    "print(len(core_file_names))\n",
    "print(len(set(core_file_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81937\n",
      "81937\n"
     ]
    }
   ],
   "source": [
    "train_path = Path(\"./data/shared-task-datasets/TIBKAT/all-subjects/data/train/\")\n",
    "dev_path = Path(\"./data/shared-task-datasets/TIBKAT/all-subjects/data/dev/\")\n",
    "all_files = list(train_path.glob(\"**/*.jsonld\"))\n",
    "all_file_names = [f.name for f in all_files]\n",
    "all_dict = {f.name: f for f in all_files}\n",
    "print(len(all_file_names))\n",
    "print(len(set(all_file_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16815\n"
     ]
    }
   ],
   "source": [
    "missed = []\n",
    "myset = set(all_file_names)\n",
    "for f in core_files:\n",
    "    if f.name not in myset:\n",
    "        missed.append(f)\n",
    "    elif f.stat().st_size != all_dict[f.name].stat().st_size:\n",
    "        print(\"ERROR: \", f)\n",
    "        \n",
    "print(len(missed))\n",
    "missed_one = missed[0]\n",
    "target = None\n",
    "for f, name in zip(core_files, core_file_names):\n",
    "    if missed_one == name:\n",
    "        target = f\n",
    "        break\n",
    "target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def md5_of_file(file_path) -> str:\n",
    "    # 创建一个md5对象\n",
    "    md5_hash = hashlib.md5()\n",
    "\n",
    "    # 以二进制模式打开文件，读取文件内容并更新到md5对象中\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        # 读取文件内容，每次读取1024字节（你可以根据需要调整这个值）\n",
    "        for chunk in iter(lambda: file.read(1024), b\"\"):\n",
    "            md5_hash.update(chunk)\n",
    "\n",
    "    # 获取十六进制格式的md5值\n",
    "    md5_value = md5_hash.hexdigest()\n",
    "    return md5_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98801\n",
      "98752\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/en/3A1702658414.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/de/3A1702658414.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/en/3A1625006101.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/de/3A1625006101.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/en/3A877616914.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/de/3A877616914.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/en/3A480350477.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/de/3A480350477.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/en/3A1773269402.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/de/3A1773269402.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/en/3A1657498700.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/de/3A1657498700.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/en/3A1665671092.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/de/3A1665671092.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/en/3A1756826285.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/de/3A1756826285.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/en/3A1666728918.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/de/3A1666728918.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/en/3A1652027823.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Book/de/3A1652027823.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A688171648.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A688171648.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A632312637.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A632312637.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A1016165137.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A1016165137.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A845671294.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A845671294.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A684988216.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A684988216.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A632072792.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A632072792.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A793002923.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A793002923.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A881977683.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A881977683.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A862540178.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A862540178.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A864285698.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A864285698.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A1000517209.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A1000517209.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A737341483.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A737341483.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A647486997.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A647486997.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A1694095584.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A1694095584.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A791044998.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A791044998.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A169418952X.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A169418952X.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A770493297.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A770493297.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A1694189422.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A1694189422.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A1008713937.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A1008713937.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A663525136.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A663525136.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A873979249.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A873979249.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A715105256.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A715105256.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A876213670.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A876213670.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A1012239659.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A1012239659.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A865584613.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A865584613.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A855608471.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A855608471.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A777433214.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A777433214.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A665751699.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A665751699.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A878911626.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A878911626.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A67163996X.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A67163996X.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A885592999.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A885592999.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A882155431.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A882155431.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A834065746.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A834065746.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A1859811094.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A1859811094.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A884121887.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A884121887.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A780075390.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A780075390.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A795361629.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A795361629.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A1008865370.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A1008865370.jsonld\n",
      "same:\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/en/3A1852845953.jsonld\n",
      "\tdata/shared-task-datasets/TIBKAT/merged-subjects/data/train/Thesis/de/3A1852845953.jsonld\n",
      "duplicate num:  49\n"
     ]
    }
   ],
   "source": [
    "train_path = Path(\"./data/shared-task-datasets/TIBKAT/merged-subjects/data/train/\")\n",
    "dev_path = Path(\"./data/shared-task-datasets/TIBKAT/merged-subjects/data/dev/\")\n",
    "merged_files = list(train_path.glob(\"**/*.jsonld\"))\n",
    "merged_file_names = [f.name for f in merged_files]\n",
    "merged_dict = {}\n",
    "print(len(merged_file_names))\n",
    "print(len(set(merged_file_names)))\n",
    "\n",
    "n_duplicate = 0\n",
    "for f in merged_files:\n",
    "    if f.name not in merged_dict:\n",
    "        merged_dict[f.name] = f\n",
    "    else:\n",
    "        existed_file = merged_dict[f.name]\n",
    "        m1 = md5_of_file(f.as_posix())\n",
    "        m2 = md5_of_file(existed_file.as_posix())\n",
    "        n_duplicate += 1\n",
    "        if m1 == m2:\n",
    "            print(f\"same:\\n\\t{f.as_posix()}\\n\\t{existed_file.as_posix()}\")\n",
    "        else:\n",
    "            print(f\"nonsame:\\n\\t{f.as_posix()}\\n\\t{existed_file.as_posix()}\")\n",
    "            \n",
    "print(\"duplicate num: \", n_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned:  0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "merged_folder = \"./data/shared-task-datasets/TIBKAT/merged-subjects/data\"\n",
    "\n",
    "merged_de_files = list(Path(merged_folder).glob(\"**/de/*.jsonld\"))\n",
    "\n",
    "n_cleaned = 0\n",
    "for f in merged_de_files:\n",
    "    en_dir = Path(f.parent.parent, \"en\")\n",
    "    en_file = Path(en_dir, f.name)\n",
    "    if en_file.exists():\n",
    "        en_file.unlink()\n",
    "        n_cleaned += 1\n",
    "print(\"cleaned: \", n_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev num:  11649\n",
      "duplicated: 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "train_path = Path(\"./data/shared-task-datasets/TIBKAT/merged-subjects/data/train/\")\n",
    "dev_path = Path(\"./data/shared-task-datasets/TIBKAT/merged-subjects/data/dev2/\")\n",
    "train_files = list(train_path.glob(\"**/*.jsonld\"))\n",
    "train_file_names = [f.name for f in train_files]\n",
    "train_file_names = set(train_file_names)\n",
    "\n",
    "dev_files = list(dev_path.glob(\"**/*.jsonld\"))\n",
    "duplicated = 0\n",
    "for f in dev_files:\n",
    "    if f.name in train_file_names:\n",
    "        duplicated += 1\n",
    "        f.unlink()\n",
    "        \n",
    "print(\"dev num: \", len(dev_files))\n",
    "print(f\"duplicated: {duplicated}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据主题嵌入相似度进行推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HELLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________\n",
      " by subject embedding query\n",
      "---------------\n",
      "Title:\t \"Städte für alle\" - über visionären und machbaren Städtebau : Martin Neuffer und Rudolf Koldewey ; Dokumentation des Symposiums am 20.06.2005 in Hannover\n",
      "Abstract:\t Zur Würdigung der beiden Oberstadtdirektoren Martin Neuffer und Rudolf Koldewey der Landeshauptstadt Hannover fand im Juni 2005 das Symposium statt. Die visionären Vorstellungen und Entwicklungsmöglichkeiten Martin Neuffers (\"Städte  für alle- Entwurf einer Städtepolitik\" als programmatischer Titel eines seiner Bücher) und die eher nüchterne und pragmatische Denk- und Handlungsweise Rudolf Koldeweys stellten das gedankliche Spannungsfeld dar, in dem die Einzelbeiträge dieser Dokumentation angesiedelt sind. Im Ergebnis werden die aktuellen Herausforderungen an Städtebau und Stadtentwicklung vor dem Hintergrund zum Teil gravierender Veränderungen der stadtentwicklungspolitischen Rahmenbedingungen aus Sicht der Kommunalpolitik, der Stadtforschung, der planenden Verwaltung und der Verwaltungsmodernisierung deutlich.\n",
      "Ground Truth:\n",
      "\t\tgnd:4073648-9 -> Kommunalpolitik\n",
      "\t\tgnd:4056795-3 -> Städtebau\n",
      "Predicted:\n",
      "\t\tgnd:4149064-2 -> Demonstrativbauvorhaben\n",
      "\t\tgnd:1079496033 -> Ab in die Mitte! Die City-Offensive Niedersachsen\n",
      "\t\tgnd:4388066-6 -> Städtebaulicher Rahmenplan\n",
      "\t\tgnd:4672568-4 -> Nachhaltige Stadtentwicklungsprojekte umsetzen\n",
      "\t\tgnd:7758371-1 -> Regionale 2013\n",
      "\t\tgnd:4700381-9 -> Stadtumbau\n",
      "\t\tgnd:7652583-1 -> Neu-Ulm 21\n",
      "\t\tgnd:7683356-2 -> Regionale 2010\n",
      "\t\tgnd:4724403-3 -> Mannheim 21\n",
      "\t\tgnd:7740548-1 -> Ab in die Mitte! Die City-Offensive NRW\n",
      "Matched Result:\n",
      "\t\tgnd:4073648-9 -> Kommunalpolitik: \tFAILURE\n",
      "\t\tgnd:4056795-3 -> Städtebau: \tFAILURE\n",
      "Methodik None\n",
      "[('Konforme Metrik', 'gnd:4829743-4'), ('s', 'gnd:4178786-9'), ('Pseudometrik', 'gnd:4176150-9')]\n"
     ]
    }
   ],
   "source": [
    "print(\"________________\\n by subject embedding query\\n---------------\")\n",
    "pred_codes, pred_names = by_similar_subjects(title, abstract, \"core\", 10)\n",
    "\n",
    "print(\"Title:\\t\", title)\n",
    "print(\"Abstract:\\t\", abstract)\n",
    "print(\"Ground Truth:\")\n",
    "for code, name in zip(true_codes, true_names):\n",
    "    print(f\"\\t\\t{code} -> {name}\")\n",
    "    \n",
    "print(\"Predicted:\")\n",
    "for code, name in zip(pred_codes, pred_names):\n",
    "    print(f\"\\t\\t{code} -> {name}\")\n",
    "    \n",
    "print(\"Matched Result:\")\n",
    "for code, name in zip(true_codes, true_names):\n",
    "    if code in pred_codes:\n",
    "        print(f\"\\t\\t{code} -> {name}: \\tSUCCESS\")\n",
    "    else:\n",
    "        print(f\"\\t\\t{code} -> {name}: \\tFAILURE\")\n",
    "        \n",
    "subject_name = \"Methodik\"\n",
    "subject_eq = SubjectEmbeddingQuery(\"./db/subject/all/\")\n",
    "mycode = subject_db.get_code_by_name(subject_name)\n",
    "print(subject_name, mycode)\n",
    "namecodes = subject_eq.get_namecodes_by_name(subject_name, 3)\n",
    "print(namecodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms4subjects.llm import LLM\n",
    "\n",
    "bot = LLM(\n",
    "    base_url=\"http://14.152.45.76:3073/v1\",\n",
    "    model=\"llama3.3:latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hallo Welt, Sommer.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "text = \"\"\"Aufgrund der steigenden Einspeisung elektrischer Leistung durch erneuerbare Energieanlagen und der gleichzeitig voranschreitenden schrittweisen Abschaltung von konventionellen Kohlekraftwerken, steigen die Anforderungen an eine zuverl\\u00e4ssige, kosteng\\u00fcnstige und klimafreundliche Bereitstellung von Flexibilit\\u00e4ten zum Erhalt der Systemstabilit\\u00e4t. Mit der Gesamtsystembetrachtung aus Strom- und Gassektor k\\u00f6nnen durch die Kopplung neue Freiheitsgrade erschlossen werden. In diesem Beitrag wird vorgestellt, wie eine sektoren\\u00fcbergreifende Erbringung von Systemdienstleistungen vom Strom- zum Gasnetz in einem dynamischen Energiesystemmodell im Zeitbereich der Mittelzeitdynamik abgebildet werden kann. Anhand von numerischen Fallstudien wird am Beispiel von Th\\u00fcringen f\\u00fcr verschiedene Szenarien ausgewertet, inwiefern durch eine sektoren\\u00fcbergreifende Betrachtung Flexibilit\\u00e4tspotenziale realisiert und die Betriebsgrenzen und somit die Stabilit\\u00e4tsbedingungen beider Sektoren eingehalten werden k\\u00f6nnen.\",\n",
    "            \"contributor\": \"Technische Informationsbibliothek (TIB)\"\"\"\n",
    "text = \"hello world, summer.\"\n",
    "response = bot.complete(f\"Please enter the text below for translation into German. If the text is already in German, then output it directly. Output only the translation result without any other auxiliary information.\\n\\n{text}\", max_tokens=2048)\n",
    "response = json.loads(response)\n",
    "response[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TIBKAT-core.jsonline', \"r\", encoding=\"utf-8\") as f:\n",
    "    items = [json.loads(line) for line in f.readlines()]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = items[0]\n",
    "e['title_DE'] = 'hello'\n",
    "e['title_EN'] = 'hello2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '3A271864087',\n",
       " 'title': 'Evaluation zur Verbesserung der Qualität der Lehre und weitere Maßnahmen',\n",
       " 'abstract': 'Die 22. AHD-Jahrestagung umfasste Kurzreferate, Werkstattseminare und Arbeitsgruppen zu weiteren wesentlichen Massnahmen: \"Hochschuldidaktische Weiterbildung\", \"Studienzeitverkuerzung\", \"Lehrberichte\", \"Materielle und immaterielle Anreize\", \"Reformstudiengaenge\", \"Kreatives Schreiben\", \"Fachuebergreifendes Lehren und Lernen\" und \"Tutorenprogramme\", \"Verbesserung der Qualitaet der Lehre durch unterschiedliche Massnahmen am Beispiel eines TEMPUS-Projekts zwischen der Universitaet Ljubljana und der FU Berlin, Arbeitsstelle Hochschuldidaktik\" und schliesslich \"Vorschlaege des BMBW zur Verbesserung der Qualitaet der Lehre\". Gliederung: I. Eroeffnung und Einfuehrung (Gerlach, Johann Wilhelm - Webler, Wolff- Dietrich - Friedrich, Hans Rainer - Thies, Erich - Berendt, Brigitte) II. - 1. Eroeffnungsreferat zur Podiumsdiskussion: Webler, Wolff-Dietrich: \"Evaluation als geeigneter Anstoss zur Reform?\" 2. Podiumsdiskussion: \"Chancen und Grenzen umfassender Evaluations-Programme\" mit Vertretern aus Universitaeten, Ministerien und Leitern/Organisatoren von Evaluations- Programmen III. Kurzreferate: Elton, Lewis: Hochschuldidaktische Weiterbildung zur Verbesserung der Qualitaet der Lehre: Aktivitaeten und Strategien in Grossbritannien. - Friedrich, Hans Rainer: Vorschlaege des BMBW zur Verbesserung der Qualitaet der Lehre und Probleme ihrer Umsetzung. - Winter, Ekkehard: Aktionsprogramm zur Studienzeitverkuerzung als Massnahme zur Verbesserung der Qualitaet der Lehre. - Klose, Traugott: Verzahnung unterschiedlicher Massnahmen zur Verbesserung der Qualitaet von Lehre und Studium am Beispiel der FU Berlin. - Marentic-Pozarnik, Barica: Verbesserung der Qualitaet der Lehre durch unterschiedliche Massnahmen am Beispiel eines TEMPUS-Projektes zwischen der Universitaet Ljubljana und der FU Berlin, Arbeitsstelle Hochschuldidaktik IV. Werkstattseminare und Arbeitsgruppen zu folgenden Themen: WS 1: Lehrevaluation an der FU Berlin - Begruendung, Methode und Techniken (Gralki, Heinz O./Gruehn, Dieter/Hecht, Heidemarie) - WS 2: Hochschuldidaktische Aus- und Weiterbildung in der Bundesrepublik Deutschland: Bestandsaufnahme und Perspektiven (Behrendt, Brigitte) - AG 1: Evaluationskonzepte in Deutschland (Buelow-Schramm, Margaret/ Reissert, Reiner) - AG 2: Studentische Evaluation (Sohr, Sven/ Stary, Joachim) - AG 3: Nicht-standardisierte Evaluations-Verfahren (Ritter,  ...',\n",
       " 'gnd_ids': [{'@id': 'http://d-nb.info/gnd/4072560-1'},\n",
       "  {'@id': 'http://d-nb.info/gnd/4241291-2'},\n",
       "  {'@id': 'http://d-nb.info/gnd/4071034-8'},\n",
       "  {'@id': 'http://d-nb.info/gnd/4013585-8'}],\n",
       " 'title_DE': 'hello',\n",
       " 'title_EN': 'hello2'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41902/41902 [00:28<00:00, 1474.93it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import re\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "dim = 1024\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "with open(\"./embedding-core.txt\", \"r\", encoding=\"utf-8\") as f_in:\n",
    "    for line in tqdm(f_in.readlines()):\n",
    "        parts = re.split(r\"[,\\t]\", line)\n",
    "        value = [float(v) for v in parts[1:]]\n",
    "        value = np.array(value, dtype=np.float32).reshape(1, dim)\n",
    "        index.add(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98752/98752 [00:05<00:00, 18540.87it/s]\n",
      "100%|██████████| 98752/98752 [00:05<00:00, 19240.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from llms4subjects.instance import instance_db_merged\n",
    "\n",
    "n = instance_db_merged.num()\n",
    "print(n)\n",
    "instance_db_merged.to_alpaca(\"./db/sft/tibkat-all.json\")\n",
    "instance_db_merged.to_alpaca(\"/root/xiatian/LLaMA-Factory/data/tibkat/tibkat-all.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98752/98752 [00:06<00:00, 14667.83it/s]\n",
      "100%|██████████| 19949/19949 [00:01<00:00, 13567.12it/s]\n",
      "100%|██████████| 118701/118701 [00:00<00:00, 612033.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from llms4subjects.instance import merge_all_to_alpaca\n",
    "\n",
    "# 包含了merged目录下的所有jsonld文件，计划在最终验证时训练该数据\n",
    "merge_all_to_alpaca(\"./db/sft/tibkat-all-2.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms4subjects-s_Ootod2-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
